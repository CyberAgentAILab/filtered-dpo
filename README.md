# Filtered Direct Preference Optimization

### tl;dr
 Introducing Filtered Direct Preference Optimization (fDPO) that enhances language model alignment with human preferences by discarding lower-quality samples compared to those generated by the learning model

## Prerequisites
- [Python 3.10.x][python]  
- [Poetry 1.7.x][poetry]  
- [direnv][direnv] 

[python]: https://www.python.org/downloads/release/python-31012/
[poetry]: https://python-poetry.org/
[direnv]: https://direnv.net/

## Get Started

```shell
cp .env.example .env
```

Edit it according to your environment and apply direnv

```shell
direnv allow .
```

### Installation
```shell
poetry install
```

### Obtain Access to Datasets and Models

- [Dataset][dataset]
- [Model][model]


[dataset]: https://huggingface.co/datasets/Mitsuki-Sakamoto/fdpo-preference-dataset
[model]: https://huggingface.co/Mitsuki-Sakamoto/fdpo-models

## Usage

### Test training

```
bash scripts/test.sh 
```

### Train 160m model

```
# $seed in {1, 2, 3}
seed=1
bash scripts/160m/fdpo_mix.sh ${seed}
```


### Train 1.4b model

```
# $seed in {1, 2, 3}
seed=1
bash scripts/1.4b/fdpo_mix.sh ${seed}
```


## Reference
Our paper is currently under review. The reference will be updated once the paper is published on arXiv.